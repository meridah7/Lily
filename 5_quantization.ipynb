{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meridah7/Lily/blob/main/5_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLNHtJrAqKWo"
      },
      "source": [
        "# **5. Quantization**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5kS114ooq-0"
      },
      "source": [
        "## 5.0 Setup Capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zjOhFCIkxVfL",
        "outputId": "e78674f3-39c4-41f4-8221-c7b3499e8aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hoEUDK6KxVfM"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username\n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lXP6DjaqxVfM",
        "outputId": "a24588f8-8e0e-44e8-d7e7-76d16002c8e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545\n",
            "/content/gdrive/MyDrive/ece5545/a2-meridah7\n",
            "fatal: Unable to create '/content/gdrive/MyDrive/ece5545/a2-meridah7/.git/index.lock': File exists.\n",
            "\n",
            "Another git process seems to be running in this repository, e.g.\n",
            "an editor opened by 'git commit'. Please make sure all processes\n",
            "are terminated then try again. If it still fails, a git process\n",
            "may have crashed in this repository earlier:\n",
            "remove the file manually to continue.\n",
            "You are not currently on a branch.\n",
            "Please specify which branch you want to merge with.\n",
            "See git-pull(1) for details.\n",
            "\n",
            "    git pull <remote> <branch>\n",
            "\n",
            "/content/gdrive/MyDrive/ece5545\n"
          ]
        }
      ],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# %mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "# !git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"meridah7@gmail.com\"\n",
        "!git config --global user.name \"meridah7\""
      ],
      "metadata": {
        "id": "DUwcHR7-IPTs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6eDWK1EVxVfN"
      },
      "outputs": [],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w6cL9l3xVfO"
      },
      "source": [
        "Please verify the cell below prints out the github repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TQSeJ7fExVfP",
        "outputId": "98c60628-f6fd-49b8-a041-7d8e62db0f7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_audio_preprocessing.ipynb\t     5_quantization.ipynb\t      images\t src\n",
            "2_size_estimator_and_profiler.ipynb  6_pruning.ipynb\t\t      models\t tests\n",
            "4_model_conversion.ipynb\t     arduino_nano_33_ble_tutorial.md  README.md\n"
          ]
        }
      ],
      "source": [
        "!ls '{PROJECT_ROOT}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM5gV8CNqd-s"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NLE4Xmakoq-4",
        "outputId": "e18c801b-51ef-4f74-d3ec-93d10317d2c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "!pip install tqdm\n",
        "!pip install torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55zgRQWVqu6n"
      },
      "source": [
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0pk0tnhRoq-5",
        "outputId": "ad2fd8a3-1ae5-4c46-dd8a-ae99f04f7d66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model folders are created, \n",
            "PyTorch models will be saved in /content/gdrive/MyDrive/ece5545/models/torch_models, \n",
            "ONNX models will be saved in /content/gdrive/MyDrive/ece5545/models/onnx_models, \n",
            "TensorFlow Saved Models will be saved in /content/gdrive/MyDrive/ece5545/models/tf_models, \n",
            "TensorFlow Lite models will be saved in /content/gdrive/MyDrive/ece5545/models/tflite_models, \n",
            "TensorFlow Lite Micro models will be saved in /content/gdrive/MyDrive/ece5545/models/micro_models.\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "\n",
        "import sys\n",
        "\n",
        "# Adding assignment 2 to the system path\n",
        "# -- make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "# Import data_proc to use data processing functions\n",
        "import src.data_proc as data_proc\n",
        "\n",
        "# Import constants to use constants defined for training\n",
        "from src.constants import *\n",
        "\n",
        "# Set random seed\n",
        "# Make sure the shuffling and picking is deterministic\n",
        "# Note that different value of random_seed may change rate of variation in loss/accuracy during training\n",
        "# Using the same random seed value every time you rerun the notebook will\n",
        "# reproduce the training and testing results\n",
        "random_seed = RANDOM_SEED\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPNyurTpoq_C"
      },
      "source": [
        "## 5.1 Define Quantization Functions\n",
        "\n",
        "There are some test cases in the `tests` folder to verify basic functionality of your implemented functions--these will be run automatically every time you check in your code. Additionally, we've left some simple tests in this notebook as well for you to try things out.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTaCRrjeJgBR"
      },
      "source": [
        "#### TODO 0: Implement the backward pass of `ste_round` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VQy4ffsVPc8o"
      },
      "outputs": [],
      "source": [
        "# add a test if you like. There's already one under tests/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOvGVC1CxVfR"
      },
      "source": [
        "\n",
        "#### TODO 1: Implement the `linear_quantize` function in `src/quant.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EXUIpaEMoq_C",
        "outputId": "90fb1c3d-3312-407e-cb1c-57e911fabf11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., -0., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "from src.quant import linear_quantize\n",
        "\n",
        "# Mini test case for linear_quantize\n",
        "with torch.no_grad():\n",
        "    x = torch.tensor([2, -0.5, 0., 1.])\n",
        "    scale = 1\n",
        "    zero = 0\n",
        "    y = linear_quantize(x, scale, zero)\n",
        "    print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvZ9zUycxVfR"
      },
      "source": [
        "#### TODO 2: Implement the `SymmetricQuantFunction` forward function in `src/quant.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RxiSaE_dxVfR",
        "outputId": "27026112-f527-4657-c574-eccc3aa5bdde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., -0., 0., 1.], grad_fn=<SymmetricQuantFunctionBackward>)\n",
            "tensor([2., -0., 0., 2.])\n"
          ]
        }
      ],
      "source": [
        "from src.quant import SymmetricQuantFunction\n",
        "\n",
        "quant_f = SymmetricQuantFunction.apply\n",
        "\n",
        "x = torch.tensor([2, -0.5, 0., 1.])\n",
        "x.requires_grad = True\n",
        "bw = 2\n",
        "y = quant_f(x, bw, scale, zero)\n",
        "(y ** 2).sum().backward()\n",
        "\n",
        "print(y)\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M_CmQ5-xVfS"
      },
      "source": [
        "#### TODO 3: Implement the `AsymmetricQuantFunction` forward function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XQBP2WWtxVfS",
        "outputId": "53f9d00f-46ad-4171-dad8-39b03c599743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., -0., 0., 1.], grad_fn=<AsymmetricQuantFunctionBackward>)\n",
            "tensor([4., -0., 0., 2.])\n"
          ]
        }
      ],
      "source": [
        "from src.quant import AsymmetricQuantFunction\n",
        "\n",
        "quant_f = AsymmetricQuantFunction.apply\n",
        "\n",
        "x = torch.tensor([2, -0.5, 0., 1.])\n",
        "x.requires_grad = True\n",
        "bw = 2\n",
        "y = quant_f(x, bw, scale, zero)\n",
        "(y ** 2).sum().backward()\n",
        "\n",
        "print(y)\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EClG-iStxVfS"
      },
      "source": [
        "#### TODO 4: Finish the Implement of `get_quantization_params` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wtjXI5zjxVfS",
        "outputId": "b38f6442-f962-4839-ec3f-5235c2cf58aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(0.1429), tensor(0.))\n",
            "(tensor(0.0667), tensor(-0.))\n"
          ]
        }
      ],
      "source": [
        "from src.quant import QConfig\n",
        "\n",
        "qconfig = QConfig(quant_bits=4, is_symmetric=True)\n",
        "print(qconfig.get_quantization_params(x.min(), x.max()))\n",
        "\n",
        "qconfig = QConfig(quant_bits=4, is_symmetric=False)\n",
        "print(qconfig.get_quantization_params(x.min(), x.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLT8xONCoq_D"
      },
      "source": [
        "#### TODO 5: Implement the `quantize_weights_bias` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1athpSf1oq_D",
        "outputId": "0e3ba7fe-38a9-46cf-eeea-2df893b9c786",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 7., -2.,  0.,  3.])\n",
            "tensor([ 7., -3.,  0.,  4.])\n"
          ]
        }
      ],
      "source": [
        "from src.quant import quantize_weights_bias, QConfig\n",
        "\n",
        "qconfig = QConfig(quant_bits=4, is_symmetric=True)\n",
        "\n",
        "w1 = nn.Parameter(torch.tensor([2, -0.5, 0., 1.]))\n",
        "qw1 = quantize_weights_bias(w1, qconfig)\n",
        "print(qw1.data)\n",
        "\n",
        "w2 = nn.Parameter(torch.tensor([2.5, -1, 0., 1.5]))\n",
        "qw2 = quantize_weights_bias(w2, qconfig)\n",
        "print(qw2.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnpBdnuBoq_E"
      },
      "source": [
        "## 5.2 Quantization Function for Linear and Convolution Layer\n",
        "\n",
        "#### TODO 6: Finish the implementation of `conv2d_linear_quantized` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XNN8WiYPoq_E",
        "outputId": "6456366f-e94d-4ea2-b70e-c0e9a691a7db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.1000, 2.1000]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[0.9571, 2.1000]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from src.quant import QuantWrapper\n",
        "\n",
        "layer = nn.Linear(2, 2)\n",
        "layer.weight.data = torch.tensor([[0.1, 0.1], [-0.1, 0.1]]).view(2, 2).float()\n",
        "layer.bias.data = torch.tensor([1, 2]).view(*layer.bias.shape).float()\n",
        "x = torch.tensor([[0., 1]])\n",
        "print(layer(x))\n",
        "\n",
        "quant_layer = QuantWrapper(\n",
        "    layer,\n",
        "    QConfig(quant_bits=4, is_symmetric=True),\n",
        "    QConfig(quant_bits=4, is_symmetric=True),\n",
        "    QConfig(quant_bits=4, is_symmetric=True))\n",
        "print(quant_layer(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lax5KDyxoq_F"
      },
      "source": [
        "## 5.3 Prepare model for QAT (Quantization Aware Training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLFhKVStmTbG"
      },
      "source": [
        "### Get Audio Processor, Devices, Data Loader, and Model\n",
        "\n",
        "NOTE: This is identical to section 2.2 ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SBcLXHHim6ry",
        "outputId": "aa771f5e-1048-4db3-cdec-9cbbfba119c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio_processor created\n",
            "Using cuda to run the training scrpit.\n",
            "Train size: 10556 Val size: 1333 Test size: 1368\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyConv(\n",
              "  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n",
              "  (conv): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_reshape): Reshape(output_shape=(-1, 4000))\n",
              "  (fc): Linear(in_features=4000, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Create audio_processor\n",
        "# DATASET_DIR is defined in constants.py\n",
        "# HINT: In case loading data takes too long, move the dataset from gdrive to /content/ and change the path accordingly.\n",
        "audio_processor = data_proc.AudioProcessor(data_dir=DATASET_DIR)\n",
        "print(\"Audio_processor created\")\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} to run the training scrpit.')\n",
        "\n",
        "# Define data loaders\n",
        "from src.loaders import make_data_loaders\n",
        "data_loaders = make_data_loaders(audio_processor, device)\n",
        "train_loader = data_loaders['training']\n",
        "test_loader = data_loaders['testing']\n",
        "valid_loader = data_loaders['validation']\n",
        "\n",
        "# Create a full precision (float32) TinyConv model\n",
        "from src.networks import TinyConv\n",
        "model_fp32 = TinyConv(model_settings=audio_processor.model_settings, \\\n",
        "    n_input=1, n_output=audio_processor.num_labels)\n",
        "\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7wcn11MxVfT"
      },
      "source": [
        "### Load Pretrained Model for Quantization Aware Finetuning\n",
        "\n",
        "In this notebook, we will load the previously trained 32-bits float model to finetune it in a quantizaiton-aware way.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "e_QYYl4oxVfT",
        "outputId": "447d4051-7d35-4582-9447-b9d71ad30e4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545\n",
            "/content/gdrive/MyDrive/ece5545/models/torch_models\n",
            "fp32_checkpoint.pt\t     quant_checkpoint.pt\n",
            "fp32_finetune_checkpoint.pt  tinyconv_float32_init_seed0_90.28%_0.pt\n"
          ]
        }
      ],
      "source": [
        "! pwd\n",
        "%cd /content/gdrive/MyDrive/ece5545/models/torch_models/\n",
        "!ls\n",
        "TORCH_DIR = \"/content/gdrive/MyDrive/ece5545/models/torch_models/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys6gVPF6xVfT"
      },
      "source": [
        "### **TODO: Replace the torch_path model with the model you created in the last section.**\n",
        "\n",
        "You can find the name of your file in `TORCH_DIR` under the folder icon to the left. (Or from running the tab above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "98TcWxPWxVfU",
        "outputId": "031fd1cc-0599-4a93-f5b0-cdf983d9b12b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-40e93926a152>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_fp32.load_state_dict(torch.load(torch_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyConv(\n",
              "  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n",
              "  (conv): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_reshape): Reshape(output_shape=(-1, 4000))\n",
              "  (fc): Linear(in_features=4000, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# TODO: Replace me!\n",
        "torch_path = os.path.join(TORCH_DIR, \"tinyconv_float32_init_seed0_90.28%_0.pt\")\n",
        "\n",
        "# Load model\n",
        "model_fp32.load_state_dict(torch.load(torch_path))\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6RTZp_yoq_F"
      },
      "source": [
        "### Define settings for weight and activation quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KrTGvZMtoq_F"
      },
      "outputs": [],
      "source": [
        "# We choose 4 bit quantization as an example because accuracy improvements will\n",
        "# be more obvious with 4-bit or lower bit quantization\n",
        "QUANT_BITS = 4\n",
        "# Settings for activations quantization: n-bit asymmetric quantization\n",
        "a_qconfig = QConfig(quant_bits=QUANT_BITS, is_symmetric=False)\n",
        "# Settings for weights quantization: n-bit symmetric quantization\n",
        "w_qconfig = QConfig(quant_bits=QUANT_BITS, is_symmetric=True)\n",
        "# Settings for bias quantization: n-bit symmetric quantization\n",
        "b_qconfig = QConfig(quant_bits=QUANT_BITS, is_symmetric=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FD7l9cNoq_F"
      },
      "source": [
        "### Prepare quantization aware training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "capV1g7loq_F",
        "outputId": "b15d8cfd-4c6a-4dd7-c58e-b804cfbc0b48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinyConv(\n",
            "  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n",
            "  (conv): QuantWrapper(\n",
            "    (module): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n",
            "  \t(activation): quant_bits=4, quant_mode=asymmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  \t(weight): quant_bits=4, quant_mode=symmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  \t(bias): quant_bits=4, quant_mode=symmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  )\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc_reshape): Reshape(output_shape=(-1, 4000))\n",
            "  (fc): QuantWrapper(\n",
            "    (module): Linear(in_features=4000, out_features=4, bias=True)\n",
            "  \t(activation): quant_bits=4, quant_mode=asymmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  \t(weight): quant_bits=4, quant_mode=symmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  \t(bias): quant_bits=4, quant_mode=symmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  )\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from src.quant import quantize_model\n",
        "qat_model_nbit = quantize_model(\n",
        "    model_fp32, a_qconfig=a_qconfig, w_qconfig=w_qconfig, b_qconfig=b_qconfig)\n",
        "\n",
        "# Print to see the model prepared for QAT\n",
        "print(qat_model_nbit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkHtQEQXoq_G"
      },
      "source": [
        "##  5.4 Finetuning\n",
        "\n",
        "In this training, we will finetune the 32-bits float pretrained model. The goal is to finetune the weights of the 32-bits float model such that the resulted model will have better accuracy after quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvZ9g2QlxVfU"
      },
      "source": [
        "### Quantization Aware Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fCC9s6dUoq_H",
        "scrolled": false,
        "outputId": "08a6c166-301b-44b9-bb82-50aa746249d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659,
          "referenced_widgets": [
            "bf63c54df6bf412e947126550cab918c",
            "14bfee6bae57442f85b1ed3ebd3a88e4",
            "1c622826e4a34727b0426861d6708f42",
            "102bbf6d45e142458013a85a7d9d057f",
            "f5a6950749984f32880ae75dfcf46079",
            "24cfd4b5a0b04c4994e2a163a187656b",
            "7bc58d43f42b44bf9fc3b164c1a7abb9",
            "c53bc3bf2def4a23abb874be8b94d604",
            "9f2c99efdc8347ffb670ff4ff66358a2",
            "794f7519ca8a4a37b35f51ad19c51ed9",
            "d20ebd26800742d29a32988d99e318ef"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#batches: 106 \n",
            "#epochs: 30 \n",
            "#total training steps: 3180\n",
            "{'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3]}]}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf63c54df6bf412e947126550cab918c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Acc Epoch 1 = 90.7%, Train loss = 0.407\n",
            "Val Acc Epoch 2 = 89.42%, Train loss = 0.388\n",
            "Val Acc Epoch 3 = 90.17%, Train loss = 0.405\n",
            "Val Acc Epoch 4 = 86.12%, Train loss = 0.423\n",
            "Val Acc Epoch 5 = 89.5%, Train loss = 0.411\n",
            "Val Acc Epoch 6 = 87.4%, Train loss = 0.476\n",
            "Val Acc Epoch 7 = 86.72%, Train loss = 0.467\n",
            "Val Acc Epoch 8 = 88.6%, Train loss = 0.438\n",
            "Val Acc Epoch 9 = 88.07%, Train loss = 0.443\n",
            "Val Acc Epoch 10 = 87.4%, Train loss = 0.46\n",
            "Val Acc Epoch 11 = 88.45%, Train loss = 0.457\n",
            "Val Acc Epoch 12 = 88.9%, Train loss = 0.434\n",
            "Val Acc Epoch 13 = 86.72%, Train loss = 0.451\n",
            "Val Acc Epoch 14 = 87.02%, Train loss = 0.489\n",
            "Val Acc Epoch 15 = 85.67%, Train loss = 0.48\n",
            "Val Acc Epoch 16 = 87.7%, Train loss = 0.484\n",
            "Val Acc Epoch 17 = 85.82%, Train loss = 0.465\n",
            "Val Acc Epoch 18 = 88.07%, Train loss = 0.462\n",
            "Val Acc Epoch 19 = 87.77%, Train loss = 0.44\n",
            "Val Acc Epoch 20 = 87.85%, Train loss = 0.447\n",
            "Val Acc Epoch 21 = 87.92%, Train loss = 0.436\n",
            "Val Acc Epoch 22 = 87.62%, Train loss = 0.443\n",
            "Val Acc Epoch 23 = 88.45%, Train loss = 0.447\n",
            "Val Acc Epoch 24 = 85.9%, Train loss = 0.484\n",
            "Val Acc Epoch 25 = 86.42%, Train loss = 0.472\n",
            "Val Acc Epoch 26 = 88.07%, Train loss = 0.478\n",
            "Val Acc Epoch 27 = 88.97%, Train loss = 0.455\n",
            "Val Acc Epoch 28 = 87.55%, Train loss = 0.469\n",
            "Val Acc Epoch 29 = 86.87%, Train loss = 0.478\n",
            "Val Acc Epoch 30 = 86.8%, Train loss = 0.478\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from src.train_val_test_utils import train, test\n",
        "from src.train_val_test_utils import create_optimizer\n",
        "\n",
        "\n",
        "def run_training(model, data_loaders, n_epoch, log_interval, optimizer, scheduler=None,\n",
        "                 save_interval=1, resume=True, checkpoint_path=None, verbose=False):\n",
        "    test_loader = data_loaders['testing']\n",
        "    with tqdm(total=n_epoch) as pbar:\n",
        "        completed_epoch = 1\n",
        "        if resume:\n",
        "            try:\n",
        "                #continue training with previous model if one exists\n",
        "                if checkpoint_path is None:\n",
        "                    raise ValueError\n",
        "                checkpoint = torch.load(checkpoint_path)\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                if scheduler is not None:\n",
        "                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "                completed_epoch = checkpoint[\"epoch\"] + 1\n",
        "                model.eval()\n",
        "                pbar.update(completed_epoch)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        for epoch in range(completed_epoch, n_epoch + 1):\n",
        "            train_iters = len(data_loaders['training'])\n",
        "            train(model, data_loaders, optimizer, epoch, device, verbose)\n",
        "            test(test_loader, model, device,\n",
        "                 epoch=None, loader_type='Test')\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            #checkpoint the model every run\n",
        "            if epoch % save_interval == 0 and checkpoint_path is not None:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None\n",
        "                }, checkpoint_path)\n",
        "\n",
        "            # Update epoch pbar\n",
        "            pbar.update(1)\n",
        "\n",
        "\n",
        "verbose = False\n",
        "log_interval = 100\n",
        "num_batches = len(train_loader)\n",
        "n_epoch = 30\n",
        "print(f'#batches: {num_batches} \\n#epochs: {n_epoch} \\n#total training steps: {num_batches * n_epoch}')\n",
        "\n",
        "# Create optimizer\n",
        "optimizer_quant = create_optimizer(model=qat_model_nbit, learning_rate=0.001)\n",
        "print(optimizer_quant.state_dict())\n",
        "\n",
        "checkpoint_path = os.path.join(TORCH_DIR, \"quant_checkpoint.pt\")\n",
        "qat_model_nbit.to(device)\n",
        "run_training(\n",
        "    model=qat_model_nbit, data_loaders=data_loaders,\n",
        "    n_epoch=n_epoch, log_interval=log_interval,\n",
        "    optimizer=optimizer_quant, scheduler=None,\n",
        "    resume=False,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8xFMjqxVfV"
      },
      "source": [
        "### Finetune the Float Model\n",
        "\n",
        "For fair comparison, we conduct the same funetuning for the float model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hx9VthdKxVfV",
        "scrolled": false,
        "outputId": "cacc7f68-7aa3-4595-e864-1f6ac4e9573e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570,
          "referenced_widgets": [
            "82976823af58408cb4fe4dfb479ebdf8",
            "1e275d0f6e1444faa5ff2798d06a1a91",
            "06b0bcd353384c5e92f6421e44f46c91",
            "e55878a081b74e69afbbb9b40dde808e",
            "5298147c18c0473f8bd872d42e600f13",
            "03936cd90dec479b829e738c4e30f800",
            "79da1a5f2216498fb0b1f9c2ab86623b",
            "bfe2aff73b1b45838256fadc5fc94ae9",
            "b442df909c80416093ed5e1ffcec729e",
            "945b4279af9f4db38b06556053cefcdc",
            "e708aa23c9c844a7ae2b18b68bbad7ac"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82976823af58408cb4fe4dfb479ebdf8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Acc Epoch 1 = 90.77%, Train loss = 0.329\n",
            "Val Acc Epoch 2 = 91.52%, Train loss = 0.323\n",
            "Val Acc Epoch 3 = 90.92%, Train loss = 0.318\n",
            "Val Acc Epoch 4 = 91.22%, Train loss = 0.311\n",
            "Val Acc Epoch 5 = 91.07%, Train loss = 0.306\n",
            "Val Acc Epoch 6 = 91.52%, Train loss = 0.303\n",
            "Val Acc Epoch 7 = 91.6%, Train loss = 0.305\n",
            "Val Acc Epoch 8 = 90.62%, Train loss = 0.308\n",
            "Val Acc Epoch 9 = 91.22%, Train loss = 0.298\n",
            "Val Acc Epoch 10 = 91.6%, Train loss = 0.298\n",
            "Val Acc Epoch 11 = 90.77%, Train loss = 0.3\n",
            "Val Acc Epoch 12 = 91.15%, Train loss = 0.299\n",
            "Val Acc Epoch 13 = 91.22%, Train loss = 0.295\n",
            "Val Acc Epoch 14 = 91.75%, Train loss = 0.289\n",
            "Val Acc Epoch 15 = 91.37%, Train loss = 0.291\n",
            "Val Acc Epoch 16 = 91.0%, Train loss = 0.3\n",
            "Val Acc Epoch 17 = 91.37%, Train loss = 0.297\n",
            "Val Acc Epoch 18 = 91.52%, Train loss = 0.288\n",
            "Val Acc Epoch 19 = 91.52%, Train loss = 0.287\n",
            "Val Acc Epoch 20 = 90.85%, Train loss = 0.297\n",
            "Val Acc Epoch 21 = 91.6%, Train loss = 0.287\n",
            "Val Acc Epoch 22 = 92.05%, Train loss = 0.295\n",
            "Val Acc Epoch 23 = 90.92%, Train loss = 0.289\n",
            "Val Acc Epoch 24 = 91.82%, Train loss = 0.294\n",
            "Val Acc Epoch 25 = 91.22%, Train loss = 0.289\n",
            "Val Acc Epoch 26 = 91.6%, Train loss = 0.291\n",
            "Val Acc Epoch 27 = 91.45%, Train loss = 0.291\n",
            "Val Acc Epoch 28 = 91.82%, Train loss = 0.289\n",
            "Val Acc Epoch 29 = 91.97%, Train loss = 0.279\n",
            "Val Acc Epoch 30 = 91.6%, Train loss = 0.285\n"
          ]
        }
      ],
      "source": [
        "# Create optimizer\n",
        "optimizer_fp32 = create_optimizer(model=model_fp32, learning_rate=0.0001)\n",
        "\n",
        "checkpoint_path = os.path.join(TORCH_DIR, \"fp32_finetune_checkpoint.pt\")\n",
        "model_fp32.to(device)\n",
        "run_training(\n",
        "    model=model_fp32, data_loaders=data_loaders,\n",
        "    n_epoch=n_epoch, log_interval=log_interval,\n",
        "    optimizer=optimizer_fp32, scheduler=None,\n",
        "    resume=False,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOIj0QUOoq_H"
      },
      "source": [
        "## 5.5 Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95C3tSGNoq_H"
      },
      "source": [
        "We will compute the accuracy of the finetuned model in train/val/test set in this section.\n",
        "Note that this is not the final accuracy we want the model to perform well on.\n",
        "We would like our quantized-aware-finetuned model to perform well when quantized into integer.\n",
        "But the training/validation/testing accuracy of these model in quantization simulation model is still worth looking at for sanity checking purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0eFxrBqCoq_H",
        "scrolled": false,
        "outputId": "64a6d335-dcf7-4078-e84d-862c7cfe03ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'make_data_loaders' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4e7fbd6092cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_val_test_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m test_time_data_loaders = make_data_loaders(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0maudio_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'make_data_loaders' is not defined"
          ]
        }
      ],
      "source": [
        "from src.train_val_test_utils import plot_acc\n",
        "\n",
        "test_time_data_loaders = make_data_loaders(\n",
        "    audio_processor, device,\n",
        "    test_batch_size=1, valid_batch_size=1,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "plot_acc(\n",
        "    test_time_data_loaders['training'], qat_model_nbit, audio_processor, device,\n",
        "    \"Training\", 'n-bit Quantized TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['validation'], qat_model_nbit, audio_processor, device,\n",
        "    \"Validation\", 'n-bit Quantized TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['testing'], qat_model_nbit, audio_processor, device,\n",
        "    'Testing', 'n-bit Quantized TinyConv', \"float\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnJHV-H_xVfV"
      },
      "outputs": [],
      "source": [
        "plot_acc(\n",
        "    test_time_data_loaders['training'], model_fp32, audio_processor, device,\n",
        "    \"Training\", 'FP32 FT TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['validation'], model_fp32, audio_processor, device,\n",
        "    \"Validation\", 'FP32 FT TinyConv', \"float\")\n",
        "acc = plot_acc(\n",
        "    test_time_data_loaders['testing'], model_fp32, audio_processor, device,\n",
        "    'Testing', 'FP32 FT TinyConv', \"float\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AehU-BsJgBV"
      },
      "source": [
        "## 5.6 Saving the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agtFnTuTJgBV"
      },
      "outputs": [],
      "source": [
        "from src.train_val_test_utils import choose_name\n",
        "from src.quant import dequantize_model\n",
        "\n",
        "# Save the qat model\n",
        "qat_model_nbit_float = dequantize_model(qat_model_nbit)\n",
        "file_name = choose_name(\"quant\")\n",
        "# You can also define your own path\n",
        "qat_torch_path = os.path.join(TORCH_DIR, f'(QAT{QUANT_BITS}bit){file_name}.pt')\n",
        "# Save the trained n-bit qat pytorch model to PATH\n",
        "torch.save(qat_model_nbit.state_dict(), qat_torch_path)\n",
        "qat_torch_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mij775vWoq_I"
      },
      "source": [
        "## 5.7 Understanding and Evaluate the Effectiveness of Quantization-Aware Training (QAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUCINlWNtQB-"
      },
      "source": [
        "### Model conversion: Quantized/Float Fine-tuning Model Converted to Integer Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odKeXjOSoq_I"
      },
      "outputs": [],
      "source": [
        "from src.quant import dequantize_model\n",
        "from src.quant_conversion import convert_to_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ssFRMhToq_I"
      },
      "outputs": [],
      "source": [
        "# Convert to quantized model\n",
        "\n",
        "# Quantized integer model of qat_model_nbit (quantized aware finetuning model)\n",
        "int_model_nbit = convert_to_int(\n",
        "    qat_model_nbit, QUANT_BITS, dtype=torch.int32)\n",
        "\n",
        "# Post quantized model of model_fp32 (full-precision finetuned model)\n",
        "post_quant_model = convert_to_int(\n",
        "    model_fp32, QUANT_BITS, dtype=torch.int32)\n",
        "\n",
        "# Floating point models of the qat_model_nbit, without QuantWrappers\n",
        "float_model_nbit = dequantize_model(qat_model_nbit)\n",
        "\n",
        "print(int_model_nbit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh2NGspeoq_I"
      },
      "outputs": [],
      "source": [
        "from src.quant_conversion import print_features\n",
        "\n",
        "# Select a sample data to see the features of it\n",
        "sample_data, _ = audio_processor.get_data_from_file(\n",
        "    audio_processor.data_index['testing'][0], BACKGROUND_FREQUENCY,\n",
        "    BACKGROUND_VOLUME_RANGE, TIME_SHIFT_SAMPLE, 'testing')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Features from Quantized QAT Model\")\n",
        "print(\"-\" * 80)\n",
        "print_features(sample_data, int_model_nbit, 'Quantized QAT Model')\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "print(\"Features from Model fp32\")\n",
        "print(\"-\" * 80)\n",
        "print_features(sample_data, model_fp32, \"Model fp32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMm7grfnoq_I"
      },
      "source": [
        "### Compare the Performance Between Integer Models from Float/Quantized-Aware Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHJaISbYoq_J"
      },
      "outputs": [],
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "\n",
        "# Compare differences in predictions\n",
        "# QAT trained floating point model vs. integer model converted from the QAT model\n",
        "# Percentage of same predictions shows how \"quantization aware\" the float point model is\n",
        "_ = compare_model(test_loader, float_model_nbit, int_model_nbit)\n",
        "_ = compare_model_mse(test_loader, float_model_nbit, int_model_nbit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNikQddjoq_J"
      },
      "outputs": [],
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "\n",
        "# Float32 model vs. integer model converted from the float32 model using post training quantization\n",
        "_ = compare_model(test_loader, model_fp32, post_quant_model)\n",
        "_ = compare_model_mse(test_loader, model_fp32, post_quant_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "5_quantization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "eed6bdaff5cc743acd241b8f3fe4c5dc3266475018a66ac615ca19ad2f28387d"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf63c54df6bf412e947126550cab918c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14bfee6bae57442f85b1ed3ebd3a88e4",
              "IPY_MODEL_1c622826e4a34727b0426861d6708f42",
              "IPY_MODEL_102bbf6d45e142458013a85a7d9d057f"
            ],
            "layout": "IPY_MODEL_f5a6950749984f32880ae75dfcf46079"
          }
        },
        "14bfee6bae57442f85b1ed3ebd3a88e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24cfd4b5a0b04c4994e2a163a187656b",
            "placeholder": "​",
            "style": "IPY_MODEL_7bc58d43f42b44bf9fc3b164c1a7abb9",
            "value": "100%"
          }
        },
        "1c622826e4a34727b0426861d6708f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c53bc3bf2def4a23abb874be8b94d604",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f2c99efdc8347ffb670ff4ff66358a2",
            "value": 30
          }
        },
        "102bbf6d45e142458013a85a7d9d057f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_794f7519ca8a4a37b35f51ad19c51ed9",
            "placeholder": "​",
            "style": "IPY_MODEL_d20ebd26800742d29a32988d99e318ef",
            "value": " 30/30 [43:12&lt;00:00, 86.63s/it]"
          }
        },
        "f5a6950749984f32880ae75dfcf46079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24cfd4b5a0b04c4994e2a163a187656b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bc58d43f42b44bf9fc3b164c1a7abb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c53bc3bf2def4a23abb874be8b94d604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f2c99efdc8347ffb670ff4ff66358a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "794f7519ca8a4a37b35f51ad19c51ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d20ebd26800742d29a32988d99e318ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82976823af58408cb4fe4dfb479ebdf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e275d0f6e1444faa5ff2798d06a1a91",
              "IPY_MODEL_06b0bcd353384c5e92f6421e44f46c91",
              "IPY_MODEL_e55878a081b74e69afbbb9b40dde808e"
            ],
            "layout": "IPY_MODEL_5298147c18c0473f8bd872d42e600f13"
          }
        },
        "1e275d0f6e1444faa5ff2798d06a1a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03936cd90dec479b829e738c4e30f800",
            "placeholder": "​",
            "style": "IPY_MODEL_79da1a5f2216498fb0b1f9c2ab86623b",
            "value": "100%"
          }
        },
        "06b0bcd353384c5e92f6421e44f46c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfe2aff73b1b45838256fadc5fc94ae9",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b442df909c80416093ed5e1ffcec729e",
            "value": 30
          }
        },
        "e55878a081b74e69afbbb9b40dde808e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_945b4279af9f4db38b06556053cefcdc",
            "placeholder": "​",
            "style": "IPY_MODEL_e708aa23c9c844a7ae2b18b68bbad7ac",
            "value": " 30/30 [41:58&lt;00:00, 83.86s/it]"
          }
        },
        "5298147c18c0473f8bd872d42e600f13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03936cd90dec479b829e738c4e30f800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79da1a5f2216498fb0b1f9c2ab86623b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfe2aff73b1b45838256fadc5fc94ae9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b442df909c80416093ed5e1ffcec729e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "945b4279af9f4db38b06556053cefcdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e708aa23c9c844a7ae2b18b68bbad7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}